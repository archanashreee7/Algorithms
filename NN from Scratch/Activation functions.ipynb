{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb4e7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Sigmoid Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
      "\n",
      "Tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "Tanh Derivative: [0.44338254 0.58781675 1.         0.58781675 0.44338254]\n",
      "\n",
      "ReLU: [0 0 0 1 2]\n",
      "ReLU Derivative: [0 0 0 1 1]\n",
      "\n",
      "Leaky ReLU: [-0.02 -0.01  0.    1.    2.  ]\n",
      "Leaky ReLU Derivative: [0.01 0.01 0.01 1.   1.  ]\n",
      "\n",
      "Softmax: [0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_result = sigmoid(x)\n",
    "sigmoid_derivative_result = sigmoid_derivative(sigmoid_result)\n",
    "\n",
    "# Hyperbolic Tangent (tanh)\n",
    "tanh_result = tanh(x)\n",
    "tanh_derivative_result = tanh_derivative(tanh_result)\n",
    "\n",
    "# Rectified Linear Unit (ReLU)\n",
    "relu_result = relu(x)\n",
    "relu_derivative_result = relu_derivative(x)\n",
    "\n",
    "# Leaky Rectified Linear Unit (Leaky ReLU)\n",
    "leaky_relu_result = leaky_relu(x)\n",
    "leaky_relu_derivative_result = leaky_relu_derivative(x)\n",
    "\n",
    "# Softmax\n",
    "softmax_result = softmax(x)\n",
    "\n",
    "# Print results\n",
    "print(\"Sigmoid:\", sigmoid_result)\n",
    "print(\"Sigmoid Derivative:\", sigmoid_derivative_result)\n",
    "\n",
    "print(\"\\nTanh:\", tanh_result)\n",
    "print(\"Tanh Derivative:\", tanh_derivative_result)\n",
    "\n",
    "print(\"\\nReLU:\", relu_result)\n",
    "print(\"ReLU Derivative:\", relu_derivative_result)\n",
    "\n",
    "print(\"\\nLeaky ReLU:\", leaky_relu_result)\n",
    "print(\"Leaky ReLU Derivative:\", leaky_relu_derivative_result)\n",
    "\n",
    "print(\"\\nSoftmax:\", softmax_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a12d9",
   "metadata": {},
   "source": [
    "ReLU has been the best activation function in the deep learning community for a long time, but Google’s brain team announced Swish as an alternative to ReLU in 2017. Research by the authors of the papers shows that simply be substituting ReLU units with Swish units improves the classification accuracy on ImageNet by 0.6% for Inception-ResNet-v2, hence, it outperforms ReLU in many deep neural nets.\n",
    "\n",
    "SWEDISH ACTIVATION FUNCTION:\n",
    "\n",
    "Mathematical formula: Y = X * sigmoid(X)\n",
    "Bounded below but Unbounded above: Y approach to constant value at X approaches negative infinity but Y approach to infinity as X approaches infinity.\n",
    "Derivative of Swish, Y’ = Y + sigmoid(X) * (1-Y)\n",
    "Soft curve and non-monotonic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a56eec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swish: [-0.23840584 -0.26894142  0.          0.73105858  1.76159416]\n",
      "Swish Derivative: [ 0.35208054  0.07232949  0.          0.92767051 -0.92164547]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def swish(x, beta=1.0):\n",
    "    return x * (1 / (1 + np.exp(-beta * x)))\n",
    "\n",
    "def swish_derivative(x, beta=1.0):\n",
    "    swish_x = swish(x, beta)\n",
    "    return swish_x + beta * x * swish_x * (1 - swish_x)\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Swish\n",
    "swish_result = swish(x)\n",
    "swish_derivative_result = swish_derivative(x)\n",
    "\n",
    "# Print results\n",
    "print(\"Swish:\", swish_result)\n",
    "print(\"Swish Derivative:\", swish_derivative_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce3f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
